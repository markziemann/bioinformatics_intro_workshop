---
title: "Bioinformatics data skills workshop - Session 8: Introduction to Slurm for high performance computing"
author: "Burnet Bioinformatics Group"
date: "`r Sys.Date()`"
output:
  html_document:
    self_contained: yes
    mode: selfcontained
    toc: true
    toc_float: true
    code_folding: show
    fig_width: 7
    fig_height: 7
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
bibliography: references.bib
csl: plos-computational-biology.csl
---

Source: https://github.com/markziemann/bioinformatics_intro_workshop

![Image credit: Futurama.](https://static.wikia.nocookie.net/enfuturama/images/8/80/Slurm-1-.jpg/revision/latest?cb=20060626052801)

## Background

Previously we ran a fairly lightweight R based workflow on HPC, but bioinformatics can
also involve computationally intensive jobs such as processing terabytes of raw data
from hundreds of samples, or characterising interactions between millions of SNPs.

In contrast to what we've done previously, this intensive work can be divided into many
smaller and more manageable steps.
These steps can be distributed across many machines in a HPC/supercomputer to expedite
the work.

HPC clusters use schedulers to decide what jobs are able to run and they are designed to
distribute resources in a fair way while optimising utilisation.
Typically, jobs are allocated until resources are at capacity and then jobs are placed in
a queue.
There are a few different schedulers including Sun Grid Engine, PBS and Slurm.
Today we'll be using Slurm.

Jargon that we'll be using:

* Head node: when you log into a HPC it is typically into a head node.
Don't run any heavy analysis here.
It is typically used for lightweight stuff like viewing, organising and transfering
files/results.

* Cluster: A bunch of computers all connected to a central scheduler.

* Batch: a set of computational tasks that will be distributed.

* Job: Non-interactive computational task - eg: a script.
Jobs are typically submitted in batches.

* Compute node: A computer in the cluster whose purpose is to run jobs.
Clusters have many compute nodes and different types such as high RAM, GPU or specific
CPU architecture.

* Cores/threads: A modern computer typically has multiple CPU cores, processors that are
able to do independent tasks.
Laptops may have 2-8 cores and high end servers may have 64 or more.
Most CPUs also have a feature called hyperthreading, which allows each CPU to process two
parallel tasks (threads).

* Parallel processing: As a modern computer has many CPU threads, jobs can be distributed
across threads of a machine.
But there is some overhead cost to this, so jobs of 8-16 threads seem to perform well and
beyond 32, there's little benefit to adding further parallel threads.

* Pipeline: A set of scripts that involves set-up of the jobs, execution and collection of
the results.

* I/O: Input/output operations, such as reading data from disk or SSD.
Given the data-intensive nature of modern bioinformatics, I/O performance is just as
important as CPU performance.

* Memory: The amount of RAM allocated for the task.
It is important that you know how much RAM will be used for your tasks, as this is a key
scarce resource on many HPC systems.

* Storage: Data storage space. This may be have certain quotas and limitations on some HPCs.

* CPU Cache: This is the amount of short term memory that is on the CPU.
A larger cache allows the CPU to do more work per cycle, and so having a higher cache
gives better performance.

Before we jump into today's task - take a look at Burnet's HPC documentation:
http://burnethpcdocs.domain.internal.burnet.edu.au/getting-started

## Overview of today's activity

In the example workflow, we'll demonstrate how to distribute BLAST searches over several
worker nodes.
But we will build up our workflow from the basics using these steps:

1. Make an interactive session to confirm a small analysis script works properly.

2. Use the scheduler to process a few small datasets.

3. Scale-up the job to complete the task efficiently.

4. Collect the results for downstream analysis.

## Part 1 - Setting up Snakemake

On the cluster, snakemake isn't installed, but we can use conda to do that.

Let's see if conda is installed

```
module avail
```

or 

```
module spider
```

Looks like miniconda is there.

```
module load miniconda
```

This will bring us our "base" environment.

We should create a named environment for a recennt Python release, where this workflow will be executed.

```
conda create -n py312 python=3.12
```

List available user environments.

```
conda env list
```

Now activate the new environment.

```
conda activate py312
```

Now snakemake can be installed here. 
The slurm plugin will allow snakemake to use the slurm scheduler which will divide the workload over the nodes of the cluster.

```
pip install snakemake
pip install snakemake-executor-plugin-slurm
```

Now try `snakemake -h` to how the help screen.

## Part 2 - Setting up cluster information

Create the folder for the configuration file to exist.

```
mkdir -p ~/.config/snakemake/slurm/
```

Now we need to create a text file.
Here, I do it with nano.

```
nano ~/.config/snakemake/slurm/config.yaml 
```

Here is the configuration that I have been using:

```
# ~/.config/snakemake/slurm/config.yaml

cluster:
  mkdir -p logs/{rule} &&
  sbatch
    --partition={resources.partition}
    --cpus-per-task={threads}
    --mem={resources.mem_mb}
    --time={resources.time}
    --job-name=smk-{rule}-{wildcards}
    --output=logs/{rule}/{rule}-{wildcards}-%j.out
    --error=logs/{rule}/{rule}-{wildcards}-%j.err
default-resources:
  - partition=standard
  - mem_mb=4000
  - time="02:00:00"
jobs: 100
use-conda: true
restart-times: 3
max-jobs-per-second: 1
max-status-checks-per-second: 10
local-cores: 1
latency-wait: 60
keep-going: true
rerun-incomplete: true
printshellcmds: true
scheduler: greedy
```

Exit and save nano with Ctrl+x.

## Part 3 - Get the "malaria" working folder

It contains the fastq sequence files, the reference genome and a few handy scripts.

```
wget https://ziemann-lab.net/public/bioinfo_workshop/malaria.zip
unzip malaria.zip 
cd malaria
ls
```

## Part 4 - Build apptainer tools

We will need to build the key dependencies that interact with the data.

I have written a script that does this automatically for us.

```
cat build_deps.sh
```

You will see that each tool has a `tag` name which gives us the version number.

You can see that apptainer can be used for individual small tools in this way, 
which is different to the use last week, where we had a monolithic image that contained
all of the apps.
Each approach has some benefits and drawbacks.

Execute the script

```
bash build_deps.sh
```

## Part 5 - Prepare the Snakefile

The `Snakefile` that you see in the downloaded malaria folder details our workflow.

Let's read through and understand it all.

## Part 6 - Index the genome

The workflow doesn't include a bwa index, so let's do that now with our bwa apptainer.

With a normal bwa install it would be run like this:

```
bwa index ref/Plasmodium_falciparum.ASM276v2.dna_sm.toplevel.fa.gz
```

But with the apptainer it is done like this:

```
apptainer run --writable-tmpfs bwa.sif index ref/Plasmodium_falciparum.ASM276v2.dna_sm.toplevel.fa.gz
```

To check that the index was generated, check the files present in the ref folder:

```
ls ref
```

You should see five new files with different suffices including amb, ann, bwt, pac and sa.

## Part 7 - Run the workflow

If everything has been set up properly so far, you can try to execute the workflow.

Firstly we will do it without SLURM scheduling, allocating 10 CPU threads on a single machine.

```
snakemake --jobs 10
```

Now we'll try running it making use of SLURM cluster, allowing distribution of jobs over
multiple nodes.

Before we do that we need to delete the recent output.

```
snakemake --delete-all-output
snakemake --executor slurm --jobs 10 --latency-wait 60
```

## Part 8 - Extending the work

The pipeline stops after generating the BAM files, but a typical genomics pipeline would
do some downstream work on it, like variant calling.
If you want to develop your skills further, I recommend you extend the pipeline further
by index the bam files with `samtools` and call variants with `bcftools`.
